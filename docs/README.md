# Cost-effective streamed data storing while maintaining efficient search/filter capabilities

These are research and development notes on my ideas and thoughts as I attempt to resolve a defined
problem in a JavaScript while looking for solutions that have the lowest overhead with regard to
memory management and runtime cost. The solution must be able to handle millions of records and
different volumes of incoming records.

While the solution for storing a stream of data, either as a capped buffer or unbound stream, the
solution is almost obvious [^circular-buffer-wiki]. But to extract value from the buffered data,
there's gotta be a solution that indexes arbitrary dimensions of records in question, allowing
search, range and subset filters. The problem is well researched and plenty of algorithms,
structures, and implementations exist. Though some of those struggle to remain efficient when
applied to browser environments and JavaScript with its memory quirks in particular.

When testing certain assumptions about maintaining an index, I ended up with a particular
pointers-like system that matches performance of lower level languages (some details on this later).
As I went further with experiments around using this pointer system for building a linked list that
stores values in sorted order, I eventually re-discovered skip list data structure
[^skip-list-wiki]. It is also one of those data structures that can be efficiently implemented in
languages like Go or C where you have better memory management capabilities. When implemented as is
in JavaScript it may easily loose its performance benefits. I applied my findings of a pointer-like
system to the skip list and got an implementation that is up to the real world challenge now.

## The Problem

You're a frontend engineers presented with a task to display certain JSON records that coming your
way from a server at certain polling speed (in batches of couple of hundreds records) or even
through a WebSocket. The payload size is unknown and hard to predict, so putting general artificial
constraints wrt how many records can be handled at the time is only going to make UX worse. Let's
assume the users are the ones to make decision whether to put the upper limit as they can change
their mind any time, reacting to the state of UI they see.

Let's say the data is displayed in a table with pagination or infinite scrolling, and it has
controls to perform text search, or filter by range of metric A, or filter by a subset of category
B, all coming from the internal value of streamed records. According to RAIL model [^rail-model],
you'd want to optimize the response, meaning the user's request to apply filter should lead to
result without a delay significant enough to be percepted.

Given modern hardware and state of art JavaScript engines optimizations, at certain volume of data
you can do just fine here by writing idiomatic JavaScript: doing `.push()`, `.shift()`, or
`.splice()` for managing data buffer, and `.filter()`, `.find()`, etc for filtering and search. This
approach however approaches its limits at considerably low real world data volumes. We're talking
about just 2000-5000 records really, regardless of their value size. Frequent use of mentioned
methods as the new data comes in reveals the essence of JavaScript memory management being heap only
and GC being your enemy number one. To build a web UI that effortlessly handles large volumes of
data while still being fully responsive to the user's input requires certain strategic planning and
careful execution.

## Keeping records in memory

Well, there isn't much of an option here other than `Array`. You receive a record, you `.push()` it
to the end of already stored records. Not much to reinvent here as you need to iterate over those
records or access them arbitrary.

Now you're being tasked to implement a limit of records being stored at any time. You start with an
empty array, you put new records in. Then you hit the limit. This doesn't mean the records stop
coming in, you just now need to do extra work to only show the last `N` records as a sort of "moving
window", since you can't just stop and only show the user those records as static data.

What would be an approach in idiomatic JavaScript? Probably to keep pushing records to the end, and
after processing a fresh batch do something like `records = records.slice(-limit)` only keeping
array of last `limit` items. Simple and easy, in terms of the code commited. But this will
triggering garbage collection cycle that may get longer at larget limits, completely freezing the
web page. Depending on the rate of incoming records, the user most likely to experience those micro
freezes during scrolling, typing, interacting with filter controls. Even if you may think that doing
`.push()` + `.shift()` may possibly have smaller damage than using splice method, you didn't get the
actual problem here.

I didn't really need to invent any sophisticated new data structures here, nor did I need to hack
around JavaScript garbage collection mechanisms. There is a data structure out there that is
designed for this exact use case: [Circular Buffer](https://en.wikipedia.org/wiki/Circular_buffer).
In JavaScript it can still be represented with a plain Array, and when the number of records added
to the array approaches the defined limit, the circular buffer starts overwriting the oldest
records, going in circles. Not only this means that we don't waste CPU cycles on `.push()` after
reaching the top limit, we also do not need to do extra memory manipulation to keep array as "last
`N` elements". GC still going to collection those records that were overwritten, but that's the
price we're paying regardless of the method and the number of records collected equals to the rate
of records consumed, not the total size of array. The implementation of circular buffer is so easy,
it's never going to be a source of bugs or maintenance issues. It's an effortless approach in
writing to an array that has exact benefits we're looking for with pretty much inexistent overhead.

## If only the sorting algorithms were faster

My most favorite algorithm is probably [Binary Search](https://en.wikipedia.org/wiki/Binary_search).
It is simple, easy, yet so efficient you just can't think how a thing can get any faster. Throw any
theoretical volumes of data at it, bam, here's the thing you're searching in it. It's really just
wiser to design your data around ability to apply binary search, instead of coming up with other
search algorithms. Same idea behind binary trees and all their variety.

So when it comes to implementing search and filter capabilities for arbitrary data, I gotta think of
a way to organize necessary dimensions or metrics of that data in the way where I can then apply
binary search. If I had a batch of records immediately at once, and I knew that the user would keep
filtering it with different params over and over, potentially applying cross filter, I would just
sort necessary values in particular order so then I can simply apply binary search: for range filter
it's just about doing two binary searches to find left most item and right most item. Similarly for
subset categorical filter, left and right bounds of a single value would give you the range of items
that share a dimension value.

But that's really only works if I got a batch of data of any volume at once, no streaming. Here,
records comming in at certain pace with unknown patterns of internal value. I cannot just append
pre-sorted lists, nor I can apply sort specifically to the new small batch and then append it to the
rest. The only option I got is to sort everything again. This becomes absolutely inefficient very
quickly. Forget about garbage collection overhead, or extra space needed for algorithms to work,
it's just `.sort()` that is going to kill the UX. No, you can't get faster than the sort algorithm
we have in JavaScript. Yes, I did test some. The native one is well done, so just accept its
performance. We gotta get back to the drawing board and think about a different way from scratch.
Something that may still be as efficient as binary search, because, nothing is better than that.

<!-- ## Linked lists and generational JavaScript trauma -->

<!-- To be continued... -->

<!-- ... why linked lists in javascript are not the same as in C -->

## References

[^circular-buffer-wiki]:
    [Circular buffer, Wikipedia article](https://en.wikipedia.org/wiki/Circular_buffer).

[^skip-list-wiki]: [Skip list, Wikipedia article](https://en.wikipedia.org/wiki/Skip_list).

[^rail-model]:
    [Measure performance with the RAIL model, web.dev article](https://web.dev/articles/rail).

[^pugh-paper]:
    ["Skip Lists: A Probabilistic Alternative to Balanced Trees", William Pugh](https://15721.courses.cs.cmu.edu/spring2018/papers/08-oltpindexes1/pugh-skiplists-cacm1990.pdf)

[^mit-lecture]:
    ["Randomization: Skip Lists", Srinivas Devadas, MIT 6.046J Design and Analysis of Algorithms, Spring 2015](https://www.youtube.com/watch?v=2g9OSRKJuzM)

[^lru-link]:
    ["Implementing an efficient LRU cache in JavaScript", Guillaume Plique](https://yomguithereal.github.io/posts/lru-cache)
